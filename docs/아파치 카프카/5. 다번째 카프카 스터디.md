4장 카프카 상세 개념 설명(4.1 ~ 4.3)
===================
1) 토픽과 파티션
   * #### 토픽 생성 시 파티션 개수 고려사항
     * #### 데이터 처리량
       * #### 컨슈머의 처리량을 늘리는 것
         * #### 서버 사양을 스케일UP 혹은 GC튜닝 활용
           * #### 컨슈머를 추가하여 병렬처리량을 늘리는 것
           * #### 파티션 개수만큼 컨슈머를 추가함
             * #### 예시) 프로듀서가 초당 1000레코드 전송하고 컨슈머가 초당 100레코드를 처리하면다면 필요한 파티션은 10개이다.(운영 환경에서 테스트 해야 함)
             * #### 파티션 구하는 공식 : 프로듀서 전송 데이터량 < 컨슈머 데이터 처리량 x  파티션 개수 <-- 이 공식이 성립해야 랙이 안 생김
             * #### 프로듀서가 보내는 데이터양을 하루, 시간, 분 단위까지 예측해서 최대치를 보낸다는 가정하에 계산하는 게 안정적임
     * #### 메시지 키 사용 여부
       * #### 메시지키를 사용하면 프로듀서가 토픽으로 데이터를 보낼 때 메시지키를 해쉬 변환하여 파티션에 매칭을 한다.
       * #### 파티션 개수가 달라지면 이미 매칭된 파티션과 메시지 키의 매칭이 깨지고 다른 파티션에 데이터가 할당됨
       * #### 즉, 순서 보장을 받지 못하기 때문에 커스텀 파티셔너를 개발하여 순서를 보장받도록 별도 조치가 필요함
     * #### 브로커, 컨슈머 영향도
       * #### 파티션은 브로커의 파일 시스템을 사용하기 때문에 늘어나면 브로커에 접근하는 파일 개수가 많아짐.
       * #### 운영체제에서 프로세스당 열 수 있는 파일 최대 개수를 제한함.
       * #### 결론은 브로커에서 접근하는 파일 개수를 안정적으로 유지하고 너무 많은 파티션이 생길 경우는 브로커를 늘리는 방안을 고려해야 함.
   * #### 토픽 정리 정책(cleanup.policy)
     * #### 첫째 delete로 데이터 완전 삭제
       * #### 명시적으로 토픽의 데이터를 삭제한다.(세그먼트 단위로 삭제 진행)
         * #### 세그먼트는 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위(파티션마다 별개로 생성됨, 이름은 내부 적재 오프셋 중 가장 작은 값으로 함)
           * #### segment.bytes옵션 : 1개의 세그먼트 크기 지정(이 옵션값보다 크기가 커지면 기존 세그먼트는 닫고 신규 세그먼트를 열어서 저장함)
           * #### 액티브 세그먼트 : 현재 데이터를 저장하기 위해 사용 중인 세그먼트
         * #### retention.ms : 밀리초 단위의 토픽의 데이터 유지시간 셋팅
           * #### 카프카는 일정 주기마다 세그먼트 파일의 마지막 수정 시간과 retention.ms를 비교하고 초과하면 세그먼트는 삭제된다.
         * #### retention.bytes : 토픽의 최대 데이터 크기
           * #### 해당 크기를 넘기면 세그먼트 파일들은 삭제된다.
     * #### 둘째 compact(압축)로 동일 메시지 키의 가장 오래된 데이터를 삭제
       * #### zip같은 압축이 아님 메시지 키별로 오래된 데이터를 삭제하는 정책
         * #### 메시지 키 기준으로 오래된 데이터를 삭제하기 때문에 1개 파티션에서 오프셋의 증가가 일정하지 않을 수 있음
         * #### 카프카 스트림즈의 KTable과 같이 메시지 키를 기반으로 데이터를 처리할 때 유용함(즉, 가장 최신 데이터를 제외한 나머지 데이터들을 삭제할 수 있기 때문)
         * #### min.cleanable.dirty.ratio 옵션값에 의해서 압축 시작 시점을 정하게 된다.
           * #### 테일 영역(이미 압축된 영역으로 클린 로그라 칭함)의 레코드 수, 헤드 영역(압축되기 전 레코드들이 있는 영역으로 더티 로그라 칭함)의 레코드 수의 비율을 뜻한다.
             * #### 예시 : 더티 레코드 개수 3 / (클린 레코드 수 3 + 더티 레코드 수 3) = 0.5 비율이 나옴
             * #### min.cleanable.dirty.ratio가 0.9 비율이면 용량을 많이 차지해서 효율이 안 좋고 0.1비율로 셋팅하면 너무 작게 설정해서 압축이 자주 일어나 브로커에 부하를 주게 됨
               * #### 즉, 적정값이 0.5가 좋음.
   * #### ISR(In-Sync-Replicas)
     * #### 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태를 의미(복제가 원할게 된 상태)
     * #### 리더 파티션과 팔로워 파티션 간 데이터 복제가 replica.lag.time.max.ms 주기를 가지고 복제가 되는지 체크한다.
       * #### 더 긴 시간 동안 데이터를 가져가지 않으면 팔로워 파티션에 문제가 있는 것으로 판단 --> ISR그룹에서 해당 팔로워 파티션 제외됨.
     * #### ISR로 잘 묶여 있으면 문제 발생 시 팔로워 파티션이 리더가 될 수 있는 자격이 있음.
       * #### 단, unclean.leader.election.enable옵션(디폴트 : false)을 true로 셋팅하면 ISR에 포함이 안 된 팔로워 파티션(동기화 엉망인 놈)도 리더로 선출할 수 있음
         * #### true이면 데이터가 유실될 수는 있으나 리더파티션이 다시 올라올 때까지 기다리지 않고 서비스가 계속 유지될 수 있는 장점이 있음.
   * #### 카프카 프로듀서
     * #### acks 옵션
       * #### acks=0은 프로듀서가 리더 파티션으로 데이터를 전송했을 때 리더 파티션으로 데이터가 저장되었는지 확인 안 함.
         * #### 데이터가 몇 번째 오프셋에 저장이 되었는지 확인할 수 없음.(즉, 전송이 실패해도 알 수 없으 retry 못함.)
         * #### 다른 옵션보다는 속도는 빠르다.
       * #### acks=1은 프로듀서로 보낸 데이터가 리더 파티션에만 정상적으로 적재되었는지 확인함.
         * #### 리더 파티션에 적재 중 오류가 발생하면 재시도 한다.
         * #### 팔로워 파티션이 아직 리더 파티션으로부터 데이터가 동기화가 안 되었는 데 리더 파티션이 장애가 발생하면 일부 데이터 유실이 발생할 수 있음.
         * #### acks=0보다는 속도가 느리다.
       * #### acks=all 또는 acks=-1은 프로듀서가 리더 파티션, 팔로워 파티션에 모두 적재되었는지 확인함.
         * #### acks=0, acks=1보다는 속도가 느리다. 단, 안전하게 전송하고 저장할 수 있음.
         * #### all로 설정하면 ISR에 포함된 파티션들만을 뜻한다.
           * #### min.insync.replicas : 리더파티션과 팔로워 파티션에 데이터가 적재되는 데 최소 ISR그룹의 파티션 개수
             * #### 값이 2이면 2개 이상의 파티션(리더, 팔로워)이 정상 적재 되었음을 확인함
             * #### 복제 개수를 3으로 셋팅하고 min.insync.replicas도 3으로 셋팅할 경우 1대만 이상이 생겨도 토픽을 더는 전송하지 않게 됨.
               * #### 안정적으로 토픽을 전송하려면 복제 개수는 3에 min.insync.replicas는 2로 설정하고 acks=all로 추천함.
     * #### 멱등성(idempotence)프로듀서
       * #### 멱등성 : 여러 번 연산을 수행하더라도 동일한 결과를 나타내는 뜻
         * #### 멱등성 프로듀서가 동일한 데이터를 여러 번 전송하더라도 카프카 클러스터에는 단 한 번만 저장됨을 의미함.
       * #### 기본 프로듀서는 동작 방식은 '적어도 한번 전달(at least once delivery)'지원함.
         * #### 즉, 한 번 이상 데이터를 적재할 수 있고 유실은 발생하지 않지만 데이터가 중복으로 발생할 수 있음.
       * #### 정확히 한 번 전달(exactly once delivery)
         * #### enable.idempotence값을 true로 설정해야 함.
           * #### retries는 기본값으로 Integer.MAX_VALUE설정되고 acks옵션은 all로 설정된다.
         * #### 멱등성 프로듀서의 PID, 시퀀스 넘버를 브로커가 받아서 이미 받은 데이터인지 아닌지 확인하고 신규이면 데이터를 적재(단 한 번만 동작)한다.
           * #### 즉 멱등성 프로듀서가 여러 번 전송은 장애 등의 이유로 중복으로 보낼 수 있으나 브로커가 이미 받은 레코드인지 확인하는 것임.
           * #### 만약 멱등성 프로듀서가 문제가 생겨서 죽고 신규 프로듀서로 시작을 하게 되면 PID가 달라지면서 브로커는 중복으로 데이터를 받을 수 있게 된다.
           * #### 시퀀스 넘버가 0부터 1씩 더한 값이 전달이 되는 데 일정하지 않은 시퀀스 넘버를 받게 되면 OutOfOrderSequenceException이 발생한다.
     * #### 트랜잭션 프로듀서
       * #### 트랜잭션 프로듀서가 다수의 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성을 만족시키기 위해서 사용됨.
         * #### 기존 프로듀서와는 다르게 동작함.
         * #### 트랜잭션 프로듀서를 사용하려면 enable.idempotence를 true설정, transactional.id를 임의의 스트링 값으로 정의, isolation.level을 read_committed로 설정한다.
           * #### 이렇게 설정하면 프로듀서, 컨슈머를 트랜잭션 처리 완료 데이터만 쓰고 잃게 됨.
           * #### 트랜잭션 프로듀서는 데이터 레코드와 함께 트랜잭션의 시작과 끝을 표현하는 트랜잭션 레코드를 한 개 더 보낸다.
           * #### 트랜잭션 컨슈머는 파티션에 저장된 트랜잭션 레코드를 보고 트랜잭션 완료(commit)되었음 확인하고 데이터를 가져감
             * #### 만약 트랜잭션 레코드가 존재하지 않으면 아직 트랜잭션이 완료 안 되었다고 생각하고 데이터를 안 가져감.
   * #### 카프카 컨슈머
     * #### 멀티 스레드 컨슈머
           



    
# 고민해볼 것 
* #### 단일, 분산 모두에서 오프셋 저장 방식이 다른 거지?
* #### 우리 회사는 미러메이커를 사용할까? 허브 앤 스포크 클러스터 방식이 실제 운영상에서 유용할까?
* #### 우리 재고에서도 커넥트를 쓸 수 있는 CASE는 없을까?
